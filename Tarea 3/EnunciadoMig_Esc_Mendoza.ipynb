{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqFk5CBeLjN0"
      },
      "source": [
        "## **MA5606 Tópicos Matemáticos en Aprendizaje de Máquinas, Redes Neuronales y Aprendizaje Profundo**\n",
        "\n",
        "### **Tarea 3: Regímenes Sobreparametrizados de Redes Neuronales**\n",
        "\n",
        "**Profesores: Claudio Muñoz y Joaquín Fontbona**\n",
        "\n",
        "**Auxiliares: Javier Maass y Diego Olguín**\n",
        "\n",
        "**Nombres integrantes: COMPLETAR**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3gTx7GbPsvw"
      },
      "source": [
        "**Instrucciones:**\n",
        "\n",
        "- **Fecha de entrega:** **TBD de 2024, a las 23:59.**\n",
        "\n",
        "- **Importante:** Si trabaja desde el link de Google Colab debe hacer un copia en su Drive antes de trabajar, de lo contrario se podrían no guardar sus códigos.\n",
        "\n",
        "- Debe entregar un Jupyter Notebook (archivo .ipynb) con sus código en Python. Le pueden ser de mucha utilidad los códigos vistos en la actividad práctica.\n",
        "\n",
        "- Sus códigos deben estar comentados y ordenados. Además, en formato texto debe colocar todas sus conclusiones y resultados pedidos que deban ser redactados.\n",
        "\n",
        "- En todos los ejercicios se le pide hacer al menos un gráfico. Los gráficos que realicen deben ser claros, con títulos y nombres en los ejes, junto con leyendas si es que corresponde."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iIXFkgDfRhf"
      },
      "source": [
        "### **Ejercicio 1: Regímenes Sobreparametrizados de Redes Neuronales**\n",
        "\n",
        "En esta última parte del curso, hemos estudiado algunos de los regímenes \"sobreparametrizados\" de redes neuronales que son más comunes en la literatura. En particular, han aparecido los conceptos de [Random Features (RF)](https://papers.nips.cc/paper_files/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html), [Neural Tangent Kernel (NTK)](https://arxiv.org/abs/1806.07572) y el régimen [Mean Field (MF)](https://arxiv.org/abs/1902.06015). Pueden ver también una buena referencia de todos en [esta review](https://arxiv.org/abs/2012.13982).\n",
        "\n",
        "En este ejercicio buscaremos ilustrar cómo se pueden observar estos regímenes en la práctica, y así también algunos fenómenos interesantes al respecto.\n",
        "\n",
        "En la tarea anterior ya vimos cómo implementar una red neuronal \"desde cero\" utilizando numpy y autograd. En este caso, usaremos uno de los frameworks más conocidos y ampliamente utilizados para la implementación de Modelos de Redes Neuronales: pytorch (instalada por defecto en Google Colab).\n",
        "\n",
        "Importemos las librerías correspondientes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WSlCg_77nwat"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Fijamos una random seed para hacer los experimentos reproducibles\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Si lo desean, pueden \"acelerar\" la velocidad de entrenamiento de sus redes neuronales\n",
        "# al utilizar la GPU de sus computadores (o cambiando el Runtime Type en Colab a uno con GPU)\n",
        "# Si esto les presenta errores extraños ligados a \"CUDA\", les recomiendo settear esta variable\n",
        "# como \"cpu\" (el todo el notebook corre sin necesidad de usar GPU).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqamtoNNXnxd"
      },
      "source": [
        "En torch trabajaremos con **tensores** en vez de arrays de NumPy. La ventaja es que estos pueden ser procesados en GPUs, lo que permite realizar cálculos mucho más rápido. Por otra parte, torch permite calcular automáticamente los gradientes de funciones que involucran tensores, a través de una implementación interna del módulo autograd. En particular, los tensores tienen un atributo **requires_grad** que determina si estos deben ser \"trackeados\" como variables con respecto a las cuales se puede derivar.\n",
        "\n",
        "Como ejemplo, consideremos la derivada de la función $f:\\mathbb{R}^{2\\times 2} \\to \\mathbb{R}$ dada por $f(x) = \\sum_{i=1}^2 \\sum_{j=1}^2 3(x_{i,j}+2)^2$ en el punto $x_0$ dado por la matriz de unos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dXM6_QftXoQF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valor de la funcion en x_0:  tensor(108., grad_fn=<SumBackward0>)\n",
            "Valor del gradiente en x_0:  tensor([[18., 18.],\n",
            "        [18., 18.]])\n"
          ]
        }
      ],
      "source": [
        "x0 = torch.ones(2, 2, requires_grad=True)\n",
        "z = 3*((x0+2)**2)\n",
        "out = z.sum()\n",
        "print('Valor de la funcion en x_0: ', out)\n",
        "out.backward()\n",
        "print('Valor del gradiente en x_0: ', x0.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C32EENyntrA"
      },
      "source": [
        "Consideraremos nuevamente una red neuronal de 1 capa oculta de $\\mathbb{R}$ en $\\mathbb{R}$, con $N$ unidades ocultas. Esto puede describirse como:\n",
        "\n",
        "$$ \\Phi_\\theta (x) = W_2 \\cdot \\sigma (W_1^T \\cdot x + b_1) + b_2$$\n",
        "\n",
        "Con un vector de parámetros $\\theta = (W_1, b_1, W_2, b_2)^T \\in \\mathbb{R}^{3N + 1}$, donde $W_1 \\in \\mathbb{R}^{1 \\times N}$, $b_1 \\in \\mathbb{R}^{N}$, $W_2 \\in \\mathbb{R}^{1 \\times N}$, $b_2 \\in \\mathbb{R}$. Sin pérdida de generalidad, podemos escribirlo en la siguiente forma:\n",
        "$$ \\Phi_\\theta (x) = \\frac{\\alpha}{N} \\sum_{k=1}^N W_2^{(k)} \\cdot \\sigma ((W_1^{(k)})^T \\cdot x + b_1^{(k)})$$\n",
        "donde explicitamos la multiplicación de las matrices involucradas, descomponiendo $W_1$, $W_2$ y $b_1$ en sus $N$ componentes (sabiendo que nuestra función de activación actúa puntualmente), y quitando $b_2$ (sabiendo que un componente 'constante' se puede lograr e.g. con $W_1=0$). Por otra parte, el $\\frac{\\alpha}{N}$ lo 'absorbe' el parámetro lineal en la salida.\n",
        "\n",
        "Esta escritura nos permitirá entender la red neuronal como un **sistema de $N$ particulas**, definidas por parámetros $\\theta_k = (W_1^{(k)}, b_1^{(k)}, W_2^{(k)}) \\in \\mathbb{R}^3$ con $k = 1, \\dots, N$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSvRQANQnL-m"
      },
      "source": [
        "Consideraremos $\\sigma(x) = \\text{sigmoid}(x)$ (aunque siéntanse libres de probar con distintas posibilidades):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QWmaye9JfRQQ"
      },
      "outputs": [],
      "source": [
        "activation_fn = torch.sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLffN_JDVhNR"
      },
      "source": [
        "Para definir modelos de redes neuronales en torch, se utiliza el submódulo **torch.nn** y, más específicamente, la clase **nn.Module**. Para definir una red neuronal, se debe heredar de nn.Module. Cada capa de la red se define en el método __init__ (usando en general módulos predefinidos de Pytorch, como nn.Linear, nn.Conv2d, etc.). Por su parte, la forma en que la red opera con el \"input\" se define dentro del método **forward**. Hacerlo de esta forma permite a torch guardar en GPU los parametros de la red y hacer que el procesamiento del input sea lo más rápido posible.\n",
        "\n",
        "En este caso, definiremos modelos para nuestros 3 regímenes sobreparametrizados de interés (nótese que incluimos un método **init_params** para asegurar que inicializamos las redes con parametros gaussianos, también incluimos un método **get_particles** para obtener las N 'partículas' que definen la red neuronal, es decir, $\\theta = (\\theta_k)_{k=1}^N \\in \\mathbb{R}^{N \\times 3}$):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF-bWKrpb7Hk"
      },
      "source": [
        "- Comenzamos definiendo el modelo de NN para el régimen de Random Features. Nótese que debemos fijar los parámetros de la primera capa, para lo cual empleamos el parámetro requires_grad. Nótese también que el factor de escala en este modelo es $\\frac{1}{N}$ (i.e. $\\alpha = 1$), como se ha visto en clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gdg4LGP5eKN4"
      },
      "outputs": [],
      "source": [
        "class RandomFeaturesModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, activation_fn):\n",
        "        super(RandomFeaturesModel, self).__init__()\n",
        "        # Nótese que N = hidden_dim.\n",
        "        # En el caso de Random Features, nuestro parámetro de escala es 1/N\n",
        "        self.scale = 1/hidden_dim\n",
        "        # Utilizamos las \"capas lineales\" que vienen implementadas en torch.\n",
        "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.output_layer = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        # También definimos la función de activación a utilizar\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "        # Inicializamos los parámetros de la red.\n",
        "        self.init_params(input_dim, hidden_dim)\n",
        "\n",
        "        # Para fijar los parámetros de la primera capa oculta (y que no se entrenen),\n",
        "        # debemos fijar requires_grad = False\n",
        "        for param in self.hidden_layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # En esta función se define el procesamiento de la red.\n",
        "        x = self.activation_fn(self.hidden_layer(x))\n",
        "\n",
        "        # Debemos multiplicar por el factor de escala!!\n",
        "        return self.scale*self.output_layer(x)\n",
        "\n",
        "    def init_params(self, input_dim, hidden_dim):\n",
        "        nn.init.normal_(self.hidden_layer.weight, mean=0.0, std=np.sqrt(2/input_dim))\n",
        "        nn.init.normal_(self.output_layer.weight, mean=0.0, std=np.sqrt(2/(hidden_dim*self.scale)))\n",
        "\n",
        "    def get_particles(self):\n",
        "        return torch.stack([self.hidden_layer.weight.clone().squeeze(), self.hidden_layer.bias.clone(), self.output_layer.weight.clone().squeeze()], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg09zLe5M0Yz"
      },
      "source": [
        "**Ejercicio 1.1: Basado en la forma de construir el modelo de RF, defina un módulo \"MeanFieldModel\", y uno \"NTKModel\"; que correspondan a los regímenes MF y NTK vistos en clases. Para esto, recuerde que ambos modelos tienen TODOS los parámetros entrenables; la diferencia es que el modelo de NTK tiene un \"scaling\" distinto, que es de $\\frac{1}{\\sqrt{N}}$ (i.e. $\\alpha = \\sqrt{N}$), en vez del $\\frac{1}{N}$ (i.e. $\\alpha =1$) utilizado por RF y MF.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucrfmXPPQ5iw"
      },
      "source": [
        "**Repuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "04ih5Aupdkgm"
      },
      "outputs": [],
      "source": [
        "class MeanFieldModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, activation_fn):\n",
        "        super(MeanFieldModel, self).__init__()\n",
        "\n",
        "        self.scale = 1/hidden_dim # Mismo factor de escala que RF.\n",
        "        self.hidden_layer = nn.Linear(input_dim, hidden_dim, bias=False)\n",
        "        self.output_layer = nn.Linear(hidden_dim, 1, bias=False) # Sin bias ahora, pues es Mean Field.\n",
        "        self.activation_fn = activation_fn\n",
        "        self.init_params(input_dim, hidden_dim) # En Mean Field, todos los parámetros son entrenables, luego no es necesario fijar requires_grad = False.\n",
        "\n",
        "    def forward(self, x): # Igual que en RF.\n",
        "        x = self.activation_fn(self.hidden_layer(x))\n",
        "        return self.scale*self.output_layer(x)\n",
        "\n",
        "    def init_params(self, input_dim, hidden_dim):\n",
        "        nn.init.normal_(self.hidden_layer.weight, mean=0.0, std=np.sqrt(2/input_dim))\n",
        "        nn.init.normal_(self.output_layer.weight, mean=0.0, std=np.sqrt(2/(hidden_dim*self.scale))) # Igual que en RF.\n",
        "        return\n",
        "\n",
        "    def get_particles(self):\n",
        "        return torch.stack([self.hidden_layer.weight.clone().squeeze(), self.output_layer.weight.clone().squeeze()], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73YSWPrNVgX7"
      },
      "outputs": [],
      "source": [
        "class NTKModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, activation_fn):\n",
        "        super(NTKModel, self).__init__()\n",
        "        # Rellenen con su código aquí\n",
        "    def forward(self, x):\n",
        "        # Rellenen con su código aquí\n",
        "        pass\n",
        "\n",
        "    def init_params(self, input_dim, hidden_dim):\n",
        "        # Rellenen con su código aquí\n",
        "        pass\n",
        "\n",
        "    def get_particles(self):\n",
        "        # Rellenen con su código aquí\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlCeOcgQfa31"
      },
      "source": [
        "#### Un problema sencillo\n",
        "En este Ejercicio consideraremos un problema \"de juguete\", en que queremos que nuestros modelos aproximen la función $f(x) = \\sin(\\pi x)$ a partir de observaciones ruidosas. Para esto, consideremos el siguiente método que nos permite generar datos sintéticos para este problema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUGMZWGUFc6O"
      },
      "outputs": [],
      "source": [
        "# Nótese que nuestro dataset está compuesto por tensores de torch.\n",
        "def generate_data(n_samples=100, noise=0.1):\n",
        "    X = np.linspace(-1, 1, n_samples).reshape(-1, 1)\n",
        "    y = np.sin(np.pi * X) + noise * np.random.randn(n_samples, 1)\n",
        "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2s23Xi6hupg"
      },
      "source": [
        "Por otro lado, consideren la siguiente función que les permitirá graficar los datos, así como, eventualmente, las predicciones de sus modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-L5GtYGhJ6L"
      },
      "outputs": [],
      "source": [
        "def plot_data_predictions(X_train, y_train, X_test, y_test,\n",
        "                     rf_predictions=None,\n",
        "                     ntk_predictions=None,\n",
        "                     mf_predictions=None):\n",
        "  plt.scatter(X_train, y_train, label='Train data', color = \"grey\", alpha=0.1)\n",
        "  plt.scatter(X_test, y_test, label='Test data', color = \"black\", alpha=0.5)\n",
        "  plt.plot(X_test, np.sin(np.pi * X_test), label='True function', color='black')\n",
        "  if rf_predictions is not None:\n",
        "    plt.plot(X_test, rf_predictions, label='Random Features', linestyle='dashed')\n",
        "  if ntk_predictions is not None:\n",
        "    plt.plot(X_test, ntk_predictions, label='Neural Tangent Kernel', linestyle='dotted')\n",
        "  if mf_predictions is not None:\n",
        "    plt.plot(X_test, mf_predictions, label='Mean Field Limit', linestyle='dashdot')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vQEKLbmh5sW"
      },
      "source": [
        "Generemos y Visualicemos los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeFIrZr0qBsq"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = generate_data(n_samples = 200, noise = 0.3)\n",
        "X_test, y_test = generate_data(n_samples=40, noise = 0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7MpHopHhioQ"
      },
      "outputs": [],
      "source": [
        "print(\"Forma de los tensores involucrados:\")\n",
        "print(\"X shape: \", X_train.shape, \"y shape: \", y_train.shape, \"\\n\")\n",
        "plot_data_predictions(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UamK930aiY4J"
      },
      "source": [
        "#### Framework de Entrenamiento Típico con PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2NHvGuFkaMh"
      },
      "source": [
        "Para entrenar modelos de NN en torch existen muchas herramientas que nos permiten implementar técnicas más avanzadas de entrenamiento, que les serán muy útiles si quieren trabajar con este tipo de modelos en la práctica (pueden ver [este link](https://pytorch.org/docs/stable/data.html), [este](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) y [este](https://pytorch.org/vision/main/transforms.html) para más información).\n",
        "\n",
        "En particular, los **Dataset** de Pytorch son una forma \"fancy\" de empaquetar los datos para que PyTorch pueda entender qué hacer con ellos. Por otro lado, esto nos permite \"leer los datos en tiempo real\", sin necesidad de tenerlos todos cargados en memoria constantemente (lo cual es muy útil cuando hay que trabajar con grandes conjuntos de datos que no caben en la RAM).\n",
        "\n",
        "En nuestro caso, para este problema simple, **TensorDataset** (que es un Dataset que viene por defecto con torch) es más que suficiente empaquetar nuestros pares (dato, etiqueta).\n",
        "\n",
        "Por otro lado, se emplean los **DataLoader** de PyTorch para iterar sobre los datos de forma \"eficiente\", permitiendo el uso de mini-batches, shuffling y paralelización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUjHJA1qG1Y2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Creamos los TensorDataset y los DataLoader de PyTorch\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)#, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)#, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXW-csCJnr17"
      },
      "source": [
        "Con esto listo, un loop de Entrenamiento-Evaluación típico de torch es el siguiente (nótese que incluimos dentro del loop una funcionalidad para retornar las partículas en cada paso del entrenamiento):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXzM2C-vGFbH"
      },
      "outputs": [],
      "source": [
        "# Loop de Entrenamiento-Evaluacion para cualquiera de nuestros modelos.\n",
        "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, epochs=1000, return_particles = True, has_get_particles = True):\n",
        "    model = model.to(device)\n",
        "    train_losses = []\n",
        "    param_changes = []\n",
        "\n",
        "    initial_particles = model.get_particles() if has_get_particles else None\n",
        "    particles = [initial_particles]\n",
        "\n",
        "    #Entrenamiento\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch.to(device))\n",
        "            loss = criterion(outputs, y_batch.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        if has_get_particles:\n",
        "          current_particles = model.get_particles()\n",
        "          if return_particles:\n",
        "            particles.append(current_particles)\n",
        "          param_change = torch.sqrt(((initial_particles - current_particles)**2).sum(axis=1)).mean()\n",
        "          param_changes.append(param_change.detach().cpu().numpy())\n",
        "          initial_particles = current_particles\n",
        "\n",
        "    # Evaluación\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            predictions = model(X_batch.to(device))\n",
        "            loss = criterion(predictions, y_batch.to(device))\n",
        "            test_loss += loss.item()\n",
        "            all_predictions.append(predictions)\n",
        "        test_loss /= len(test_loader)\n",
        "        all_predictions = torch.cat(all_predictions).cpu().numpy()\n",
        "\n",
        "    return all_predictions, test_loss, train_losses, param_changes, particles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js3s6zS1ph1g"
      },
      "source": [
        "#### Entrenamiento de Modelos para este problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbJm9Aqjq-uk"
      },
      "source": [
        "En el caso de nuestras redes **sobreparametrizadas**, nos interesará ver qué pasa para diferentes valores de $N$. También, dependiendo del régimen considerado, la teoría vista en el curso es aplicable en la medida que la cantidad de épocas que se ocupen sea suficiente (e.g. el Régimen MF requiere de un entrenamiento con $N\\cdot T$ épocas para estar en el scaling 'correcto') o que las constantes tengan valores razonables (e.g. el régimen de NTK requiere un learning rate 'pequeño' para que la \"linealización\" de la acción de la red tenga sentido). Definamos, ante todo, los hiperparámetros globales del entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvHVEF5ir2hW"
      },
      "outputs": [],
      "source": [
        "input_dim = 1\n",
        "LRs = (0.5, 0.01, 1)\n",
        "WDs = (0, 0, 0)\n",
        "\n",
        "N = 1000\n",
        "T = 1\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "EPOCHS = (int(N*T), int(np.sqrt(N)*T), int(N*T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBvURsUGt85i"
      },
      "source": [
        "Entrenando RF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebm0kaWvt-uM"
      },
      "outputs": [],
      "source": [
        "rf_model = RandomFeaturesModel(input_dim, N, activation_fn)\n",
        "rf_optimizer = optim.Adam(filter(lambda p: p.requires_grad, rf_model.parameters()), lr=LRs[0], weight_decay=WDs[0])\n",
        "rf_predictions, rf_loss, rf_train_losses, rf_param_changes, particles_rf = train_and_evaluate(rf_model, train_loader, test_loader, criterion, rf_optimizer, epochs = EPOCHS[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQgLg7vDuIv-"
      },
      "source": [
        "Entrenando NTK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuy9Au5wuIeg"
      },
      "outputs": [],
      "source": [
        "ntk_model = NTKModel(input_dim, N, activation_fn)\n",
        "ntk_optimizer = optim.Adam(filter(lambda p: p.requires_grad, ntk_model.parameters()), lr=LRs[1], weight_decay=WDs[1])\n",
        "ntk_predictions, ntk_loss, ntk_train_losses, ntk_param_changes, particles_ntk = train_and_evaluate(ntk_model, train_loader, test_loader, criterion, ntk_optimizer, epochs = EPOCHS[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nMReL3Iult4"
      },
      "source": [
        "Entrenando MF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "athY2W-CpuIC"
      },
      "outputs": [],
      "source": [
        "mf_model = MeanFieldModel(input_dim, N, activation_fn)\n",
        "mf_optimizer = optim.Adam(filter(lambda p: p.requires_grad, mf_model.parameters()), lr=LRs[2], weight_decay=WDs[2])\n",
        "mf_predictions, mf_loss, mf_train_losses, mf_param_changes, particles_mf = train_and_evaluate(mf_model, train_loader, test_loader, criterion, mf_optimizer, epochs = EPOCHS[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq1KERpAdVz9"
      },
      "source": [
        "**Ejercicio 1.2: Defina una función train_everything que reciba los hiperparámetros de entrenamiento y retorne, por una parte, un diccionario de llaves 'RF', 'NTK' y 'MF', cuyos valores sean los resultados de un entrenamiento bajo los hiperparámetros definidos; y por otra parte un diccionario con los modelos finales obtenidos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSEpqlvvfi1R"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6-Kob57fieV"
      },
      "outputs": [],
      "source": [
        "def train_everything(N = 1000,\n",
        "                     T = 1,\n",
        "                     input_dim = 1,\n",
        "                     LRs = (0.5, 0.001, 1),\n",
        "                     WDs = (0, 0, 0),\n",
        "                     criterion = nn.MSELoss(),\n",
        "                     EPOCHS = (int(1000*1), int(np.sqrt(1000)*1), int(1000*1)),\n",
        "                     activation_fn = torch.sigmoid,\n",
        "                     ):\n",
        "    out = {}\n",
        "    models = {}\n",
        "\n",
        "    # Rellene con su código aquí\n",
        "\n",
        "    return out, models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7wnXathH-e"
      },
      "source": [
        "Utilicemos nuestra función train_everything para probar diferentes configuraciones y ver qué pasa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrHqo5IWhm1-"
      },
      "outputs": [],
      "source": [
        "out, models = train_everything(N = N, T = T, input_dim = input_dim, LRs = LRs, WDs = WDs, criterion = criterion, EPOCHS = EPOCHS, activation_fn = activation_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbt5NRUihfmp"
      },
      "outputs": [],
      "source": [
        "rf_predictions, rf_loss, rf_train_losses, rf_param_changes, particles_rf = out[\"RF\"]\n",
        "ntk_predictions, ntk_loss, ntk_train_losses, ntk_param_changes, particles_ntk = out[\"NTK\"]\n",
        "mf_predictions, mf_loss, mf_train_losses, mf_param_changes, particles_mf = out[\"MF\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YQ9mlBE5nDj"
      },
      "source": [
        "Podemos entender cuál de los modelos obtuvo la mejor pérdida sobre el conjunto de test para los hiperparámetros definidos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgffyijOkFPm"
      },
      "outputs": [],
      "source": [
        "# Print test losses\n",
        "print(f'Random Features Test Loss: {rf_loss:.4f}')\n",
        "print(f'Neural Tangent Kernel Test Loss: {ntk_loss:.4f}')\n",
        "print(f'Mean Field Limit Test Loss: {mf_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTuAACte6Gdg"
      },
      "source": [
        "Asimismo, utilizamos la función plot_data_predictions para visualizar los resultados obtenidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l4dda3EjtWL"
      },
      "outputs": [],
      "source": [
        "plot_data_predictions(X_train, y_train, X_test, y_test, rf_predictions, ntk_predictions, mf_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ5BF7qw6kXC"
      },
      "source": [
        "**Ejercicio 1.3: Describa los resultados obtenidos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEExEOxd6pXV"
      },
      "source": [
        "**Respuesta:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubl_7QrhUhmV"
      },
      "source": [
        "#### Entendiendo el Entrenamiento bajo los diferentes regímenes sobreparametrizados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9VmWPgV7EZy"
      },
      "source": [
        "Las siguientes funciones nos permitirán entender cómo evolucionaron las losses a lo largo del entrenamiento, y así también qué tanto cambian los parámetros de la red a lo largo del entrenamiento. Nótese que, para ver los distintos regímenes de entrenamiento en escalas comparables, estamos \"reescalando los vectores de losses\" para ponerlos todos en una misma escala temporal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQQqA2OWE4He"
      },
      "outputs": [],
      "source": [
        "# Función para visualizar el comportamiento de las redes durante el entrenamiento\n",
        "def plot_training_metrics(train_losses_list, param_changes_list, labels, scales = EPOCHS, T = 1):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Graficamos la evolución de la loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for j, (train_losses, label) in enumerate(zip(train_losses_list, labels)):\n",
        "        ws = max(int(scales[j]/T), 1)\n",
        "        series = torch.nn.functional.avg_pool1d(torch.tensor(train_losses).unsqueeze(0).unsqueeze(1), kernel_size=ws, stride=ws).squeeze()\n",
        "        plt.plot(series, label=label)\n",
        "    plt.xlabel('Abstract Time')\n",
        "    plt.ylabel('Training Loss')\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    plt.title('Training Losses')\n",
        "\n",
        "    # Graficamos los \"cambios\" en los parámetros\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for j, (param_changes, label) in enumerate(zip(param_changes_list, labels)):\n",
        "        ws = max(int(scales[j]/T),1)\n",
        "        series = torch.nn.functional.avg_pool1d(torch.tensor(np.stack(param_changes)).unsqueeze(0).unsqueeze(1), kernel_size=ws, stride=ws).squeeze()\n",
        "        plt.plot(series, label=label)\n",
        "\n",
        "    plt.xlabel('Abstract Time')\n",
        "    plt.ylabel('Parameter Variation')\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    plt.title('Parameter Variations')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m45X7T0grrT"
      },
      "outputs": [],
      "source": [
        "labels = [\"Random Features\", \"Neural Tangent Kernel\", \"Mean Field Limit\"]\n",
        "plot_training_metrics([rf_train_losses, ntk_train_losses, mf_train_losses],\n",
        "                      [rf_param_changes, ntk_param_changes, mf_param_changes],\n",
        "                      labels, scales = EPOCHS, T = 15*T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqbFzN6qEYz8"
      },
      "source": [
        "La siguiente función permite realizar una animación de la evolución de las partículas durante el entrenamiento (viéndolas en la escala que corresponda)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fobI6dsdPuQF"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_particles_animation(particle_snapshots, p_size = 5,  lims = (10,10,10), scale = 1, T = 1, method = \"\"):\n",
        "    \"\"\"\n",
        "    Crea una animación de la posición de los parámetros de la red en un gráfico de parámetros 3D.\n",
        "\n",
        "    Parámetros:\n",
        "    particle_snapshots (lista de torch.Tensor): Lista de tensores, cada uno de shape (N, 3),\n",
        "                                                representando la posicion de los parametros de la red en cada época.\n",
        "    \"\"\"\n",
        "    ws = max(int(scale/T),1)\n",
        "    particle_snapshots = particle_snapshots[::ws] # 'Vemos' las partículas en la escala correspondiente.\n",
        "    frames = []\n",
        "    N = particle_snapshots[0].shape[0]\n",
        "\n",
        "    for i, particles in enumerate(particle_snapshots):\n",
        "        x = particles[:, 0].detach().cpu().numpy()\n",
        "        y = particles[:, 1].detach().cpu().numpy()\n",
        "        z = particles[:, 2].detach().cpu().numpy()\n",
        "\n",
        "        frames.append(go.Frame(data=[go.Scatter3d(\n",
        "            x=x,\n",
        "            y=y,\n",
        "            z=z,\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=p_size,\n",
        "                color=z,  # El color lo determina el valor de w_2\n",
        "                colorscale='Viridis',\n",
        "                opacity=0.8\n",
        "            )\n",
        "        )], name=f'frame{i}'))\n",
        "\n",
        "    x = particle_snapshots[0][:, 0].detach().cpu().numpy()\n",
        "    y = particle_snapshots[0][:, 1].detach().cpu().numpy()\n",
        "    z = particle_snapshots[0][:, 2].detach().cpu().numpy()\n",
        "\n",
        "    # Se ocupan muchos tecnicismos propios de plotly, no es necesario que entiendan este código.\n",
        "    fig = go.Figure(\n",
        "        data=[go.Scatter3d(\n",
        "            x=x,\n",
        "            y=y,\n",
        "            z=z,\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=p_size,\n",
        "                color=z,\n",
        "                colorscale='Viridis',\n",
        "                opacity=0.8\n",
        "            )\n",
        "        )],\n",
        "        layout=go.Layout(\n",
        "            title=f'Animacion de la Evolución de la NN durante el Entrenamiento <br><sup>Método: {method}</sup>',\n",
        "            scene=dict(\n",
        "                xaxis_title='w1',\n",
        "                yaxis_title='b1',\n",
        "                zaxis_title='w2',\n",
        "                xaxis_range=[-lims[0],lims[0]],\n",
        "                yaxis_range=[-lims[1],lims[1]],\n",
        "                zaxis_range=[-lims[2],lims[2]],\n",
        "                aspectmode ='cube',\n",
        "            ),\n",
        "            updatemenus=[{\n",
        "                'buttons': [\n",
        "                    {\n",
        "                        'args': [None, {'frame': {'duration': 100, 'redraw': True}, 'fromcurrent': True}],\n",
        "                        'label': 'Play',\n",
        "                        'method': 'animate'\n",
        "                    },\n",
        "                    {\n",
        "                        'args': [[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate', 'transition': {'duration': 0}}],\n",
        "                        'label': 'Pause',\n",
        "                        'method': 'animate'\n",
        "                    }\n",
        "                ],\n",
        "                'direction': 'left',\n",
        "                'pad': {'r': 10, 't': 87},\n",
        "                'showactive': False,\n",
        "                'type': 'buttons',\n",
        "                'x': 0.1,\n",
        "                'xanchor': 'right',\n",
        "                'y': 0,\n",
        "                'yanchor': 'top'\n",
        "            }],\n",
        "            sliders=[{\n",
        "                'steps': [{\n",
        "                    'args': [[f'frame{k}'], {'frame': {'duration': 0, 'redraw': True}, 'mode': 'immediate', 'transition': {'duration': 0}}],\n",
        "                    'label': str(k),\n",
        "                    'method': 'animate'\n",
        "                } for k in range(len(particle_snapshots))],\n",
        "                'active': 0,\n",
        "                'yanchor': 'top',\n",
        "                'xanchor': 'left',\n",
        "                'currentvalue': {\n",
        "                    'font': {'size': 20},\n",
        "                    'prefix': 'Frame:',\n",
        "                    'visible': True,\n",
        "                    'xanchor': 'right'\n",
        "                },\n",
        "                'transition': {'duration': 0},\n",
        "                'pad': {'b': 10, 't': 50},\n",
        "                'len': 0.9,\n",
        "                'x': 0.1,\n",
        "                'y': 0\n",
        "            }]\n",
        "        ),\n",
        "        frames=frames\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DJy1i5VPyoB"
      },
      "outputs": [],
      "source": [
        "plot_particles_animation(particles_rf, p_size=2, lims = (6,1,150), scale = EPOCHS[0], T= 15*T, method=\"Random Features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdB8nWGqZ-G8"
      },
      "outputs": [],
      "source": [
        "plot_particles_animation(particles_ntk, p_size = 2, lims = (10,7,1), scale = EPOCHS[1], T= 15*T, method = \"Neural Tangent Kernel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBQnt7ekGCst"
      },
      "outputs": [],
      "source": [
        "plot_particles_animation(particles_mf, p_size = 2, lims = (200,200,200), scale = EPOCHS[2], T= 15*T, method=\"Mean Field\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzM8oX1pL403"
      },
      "source": [
        "**Ejercicio 1.4: Haga un breve análisis de lo que se observa durante el entrenamiento a nivel de pérdidas, variación de parámetros y la posición de las partículas para cada método (en particular, enfatice lo que diferencia a cada uno de los regímenes). Ve algún fenómeno de \"lazy-training\" bajo alguno de los regímenes?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZIZSrbAL5dP"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGvEnHZdUrTY"
      },
      "source": [
        "#### Búsqueda de Hiperparámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT7tPvJ6MbdB"
      },
      "source": [
        "**Ejercicio 1.5: Repita los experimentos anteriores para 3 configuraciones distintas de parámetros (e.g. variando los LRs, considerando WDs >0, cambiando el método de optimización (pueden ver [aquí](https://pytorch.org/docs/stable/optim.html) algunos algoritmos de referencia), cambiando la función de pérdida (pueden ver [aquí](https://pytorch.org/docs/stable/nn.html#loss-functions) algunas de referencia), variando T; etc.). Tras esto, defina una buena configuración de hiperparámetros, la cual utilizaremos en la siguiente pregunta.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smlp7e6nMbRC"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ne-dS71VMmJ"
      },
      "outputs": [],
      "source": [
        "# Rellene con su código aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sW3rhwJUuqw"
      },
      "source": [
        "#### Estudio de la importancia de $N\\to\\infty$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIZt4jNB_Dyo"
      },
      "source": [
        "**Ejercicio 1.6: Queremos visualizar el comportamiento de las redes con $N\\to \\infty$. Para esto, fije un valor de $T$ que le parezca conveniente, y entrene redes con RF, NTK y MF, para $N=[5, 10, 50, 100, 500, 1000, 5000]$ (recuerde que la cantidad de \"épocas de entrenamiento\" depende de este valor de $N$). Repita cada experimento al menos $N_{reps} = 5$ veces. Con esta información, grafique la test_loss obtenida para los diferentes regímenes y para los distintos valores de $N$ (incluyendo barras de error). Le puede ser muy útil la función train_everything definida anteriormente.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrgFmyIWAmUc"
      },
      "source": [
        "Para esto, le será útil la siguiente función para graficar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb8jNAduAtGU"
      },
      "outputs": [],
      "source": [
        "def plot_losses_with_error_bars(N_values, rf_losses, ntk_losses, mf_losses):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        N_values: Lista de valores de N utilizados en los experimentos.\n",
        "        rf_losses: Diccionario de losses para Random Features.\n",
        "        ntk_losses: Diccionario de losses para Neural Tangent Kernel.\n",
        "        mf_losses: Diccionario de losses para Mean Field.\n",
        "        Estos diccionarios de losses deben ser de la forma: {N: [lista_de_losses]}\n",
        "        i.e. las llaves son los distintos valores de N empleados, y el objeto\n",
        "        asociado es una lista con las losses correspondientes a cada una de las\n",
        "        repeticiones del experimento.\n",
        "    \"\"\"\n",
        "    rf_means = [np.mean(rf_losses[N]) for N in N_values]\n",
        "    rf_stds = [np.std(rf_losses[N]) for N in N_values]\n",
        "\n",
        "    ntk_means = [np.mean(ntk_losses[N]) for N in N_values]\n",
        "    ntk_stds = [np.std(ntk_losses[N]) for N in N_values]\n",
        "\n",
        "    mf_means = [np.mean(mf_losses[N]) for N in N_values]\n",
        "    mf_stds = [np.std(mf_losses[N]) for N in N_values]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.errorbar(N_values, rf_means, yerr=rf_stds, label='Random Features', marker='o', capsize=5)\n",
        "    plt.errorbar(N_values, ntk_means, yerr=ntk_stds, label='Neural Tangent Kernel', marker='o', capsize=5)\n",
        "    plt.errorbar(N_values, mf_means, yerr=mf_stds, label='Mean Field', marker='o', capsize=5)\n",
        "\n",
        "    plt.xlabel('N')\n",
        "    plt.ylabel('Test Loss')\n",
        "    plt.xscale(\"log\")\n",
        "    plt.title('Test Loss vs. N for Different Models with Error Bars')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkS4xD7xSyVf"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH3XBp-exKxg"
      },
      "outputs": [],
      "source": [
        "# Rellene con su código aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrlDis1qxKvH"
      },
      "outputs": [],
      "source": [
        "# Rellene con su código para graficar con barras de error.\n",
        "plot_losses_with_error_bars(N_values, rf_losses, ntk_losses, mf_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASf1ccpdOcEH"
      },
      "source": [
        "**Ejercicio 1.7: Describa los resultados obtenidos, y explíquelos desde la perspectiva de los conceptos estudiados en el curso (e.g. sobreparametrización, overfitting, aproximación universal, etc.)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XQj2SC4O4Wl"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnTibp3MPOzA"
      },
      "source": [
        "### **Ejercicio 2: Aplicación de ideas de _sobreparametrización_ en un problema \"real\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8fY-sZbPp0t"
      },
      "source": [
        "En esta sección implementaremos un método de clasificación de series de tiempo (llamado MiniRocket, véase este [link](https://arxiv.org/abs/2012.08791)) para intentar resolver un problema \"real\". Para esto, utilizaremos una implementación del método MiniRocket de la librería sktime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIqzsjDtjvxS"
      },
      "outputs": [],
      "source": [
        "!pip install sktime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wecD4IXiRrfu"
      },
      "source": [
        "El algoritmo de MiniRocket es una variante más ligera y eficiente del algoritmo ROCKET (ver este [link](https://arxiv.org/abs/1910.13051)), el cual guarda muchísimas similitudes con el modelo de Random Features (RF) que trabajamos en el ejercicio anterior.\n",
        "\n",
        "En el modelo de RF que usamos, dado que la primera capa de la red está \"fija\" (en valores aleatorios para cada peso), la acción de la red es equivalente a \"extraer $N$ características aleatorias a partir del dato de entrada\" (con la primera capa) y luego clasificarlas utilizando un modelo lineal (con la segunda capa, que es entrenable).\n",
        "\n",
        "En el caso de MiniRocket, los datos de entrada corresponden a series de tiempo que son, en principio, más complejas. Siguiendo una lógica similar a RF, el método extrae una **gran cantidad de características aleatorias** a partir de la serie de tiempo (en este caso, utilizando _convoluciones aleatorias_ mejor adaptadas a este tipo de dato) y luego las clasifica utilizando un modelo lineal. Para más información pueden ver los papers originales ([ROCKET](https://arxiv.org/abs/1910.13051) y [MiniRocket](https://arxiv.org/abs/2012.08791)).\n",
        "\n",
        "La idea de este ejercicio será simplemente utilizar estos algoritmos para resolver un problema aplicado. Noten que la metodología que emplearemos es aplicable a todo tipo de problema de clasificación de series de tiempo, y quedan cordialmente invitad@s a intentar utilizarla en los problemas que les aparezcan a futuro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75OxfNBNvt9M"
      },
      "source": [
        "Importemos las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YODtfIIpjvuh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sktime.transformations.panel.rocket import MiniRocket\n",
        "from sktime.classification.kernel_based import RocketClassifier\n",
        "from sktime.datasets import load_basic_motions, load_italy_power_demand, load_acsf1, load_UCR_UEA_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ8BOVyqwzgd"
      },
      "source": [
        "#### Un problema \"real\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7vIB5Biw3ED"
      },
      "source": [
        "Utilizaremos nuestro brillante algoritmo de clasificación de series de tiempo (TSC) sobre el dataset **SelfRegulationSCP1** de la base de datos abierta de UCR (ver [aquí](https://www.timeseriesclassification.com/index.php) para mayores referencias).\n",
        "\n",
        "Los datos corresponden a registros de ElectroeEncefaloGrama (EEG) de un sujeto saludable, al que se le pidió mover un cursor arriba y abajo en la pantalla de un computador (mientras se medían sus potenciales corticales).\n",
        "\n",
        "Al sujeto se le midieron '6 canales' de su señal cerebral (A1-Cz, A2-Cz, 2cm frontal de C3, 2 cm parietal de C3, 2 cm frontal de C4 y 2 cm parietal de C4), en múltiples mediciones de 3.5 segundos, sampleadas a una frecuencia de 256 Hz. Esto se traduce en que tenemos múltiples series de tiempo de \"6 canales\" (i.e. tiene 6 dimensiones) de largo 896 cada una. Todos los valores están medidos en microVolts.\n",
        "\n",
        "Cada serie de tiempo está clasificada según el movimiento del cursor que imprimía el sujeto. En particular, la _positividad cortical_ corresponde a un movimiento descendente del cursor, mientras que la negatividad cortical corresponde a uno ascendente.\n",
        "\n",
        "En total, hay  268 muestras de entrenamiento y 293 muestras de prueba para este problema, a las cuales podemos acceder a través de la funciones integradas de sktime:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I4dEsQpvdjW"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = load_UCR_UEA_dataset(name=\"SelfRegulationSCP1\", split=\"train\", return_X_y=True, return_type = \"numpy3D\")\n",
        "X_test, y_test = load_UCR_UEA_dataset(name=\"SelfRegulationSCP1\", split=\"test\", return_X_y=True, return_type = \"numpy3D\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptB3_kTa2ONr"
      },
      "source": [
        "Estos datos vienen estructurados como tensores de orden 3, donde la primera dimensión corresponde a las diferentes muestras ($B$), la segunda al número de canales ($C$) y la tercera al largo de la serie de tiempo ($L$):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhFtRSvE2Cva"
      },
      "outputs": [],
      "source": [
        "print(\"Train Data Shape (B, C, L):\", X_train.shape)\n",
        "print(\"Test Data Shape (B, C, L):\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApbX81pG3k2a"
      },
      "source": [
        "Para visualizar estos datos, consideren la siguiente función que samplea aleatoriamente algunas de las muestras de las series de tiempo y las grafica junto con su clase. El gráfico en el eje x tiene el tiempo, en el eje y la intensidad de la señal (en microVolts) y cada uno de los 6 canales de la señal se representa con un color distinto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86O0TFNbvT3G"
      },
      "outputs": [],
      "source": [
        "def plot_sample_time_series(X, y, num_samples=3, title=\"Algunos Ejemplos de las Series de Tiempo\"):\n",
        "    plt.figure(figsize=(8, num_samples*2.4))\n",
        "    num_channels = len(X[0])\n",
        "    channel_names = [\"A1-Cz\", \"A2-Cz\", \"2cmfC3\", \"2cmpC3\", \"2cmfC4\", \"2cmpC4\"]\n",
        "    indices = np.random.choice(range(len(X)),num_samples, replace=False)\n",
        "    lims = X.max()\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(num_samples, 1, i + 1)\n",
        "        for ch in range(num_channels):\n",
        "            plt.plot(X[indices[i]][ch], label=channel_names[ch])\n",
        "        plt.title(f\"Muestra {i+1}, Clase: {y[indices[i]]}\")\n",
        "        plt.xlabel(\"Tiempo\")\n",
        "        plt.ylabel(\"Intensidad\")\n",
        "        plt.ylim((-0.8*lims, 0.8*lims))\n",
        "        if i ==0:\n",
        "          plt.legend(loc=\"upper right\")\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4YZZaeV5mXo"
      },
      "source": [
        "Vemos algunos ejemplos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWpyzE7P5js7"
      },
      "outputs": [],
      "source": [
        "plot_sample_time_series(X_train, y_train, num_samples = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0o5JQteCjUh"
      },
      "source": [
        "#### Un modelo de \"Random Features\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2FsEwsc6Tfo"
      },
      "source": [
        "El módulo de MiniRocket nos permite, a partir de nuestros datos de series de tiempo, obtener una matriz de muchas \"features aleatorias\" (de la forma $(B,F)$, con $F$ el número de _features_ extraídas). Esto se realiza utilizando una estructura muy similar a los modelos tradicionales de sklearn:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4gcqBrwBxF1"
      },
      "source": [
        "**Ejercicio 2.1: Defina una instancia del modelo de MiniRocket basándose en el framework de sktime (vean este [link](https://www.sktime.net/en/stable/examples/transformation/minirocket.html) para más información), y utilícelo para obtener matrices de características X_train_transform y X_test_transform, a partir de X_train y X_test respectvamente.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPVQXTLHT9pb"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHi5Hce2vaLd"
      },
      "outputs": [],
      "source": [
        "# Rellene con su código aquí:\n",
        "\n",
        "# Defina su estimador\n",
        "\n",
        "# Hagale .fit() con los datos de entrenamiento\n",
        "\n",
        "# Obtenga X_train_transform y X_test_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHaqQtb17nIX"
      },
      "source": [
        "Obtenemos la forma de la matriz de \"features aleatorias\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHpkrX6v7ovP"
      },
      "outputs": [],
      "source": [
        "print(\"Train Data Shape (B, F):\", X_train_transform.shape)\n",
        "print(\"Test Data Shape (B, F):\", X_test_transform.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_RemygW7zDa"
      },
      "source": [
        "También les damos una función para visualizar aproximadamente las _features_ obtenidas para algunas muestras de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH1zl5FRvbyc"
      },
      "outputs": [],
      "source": [
        "def plot_transformed_features(X_transform, y, num_samples = 10, title=\"Ejemplos de las Features Aleatorias obtenidas\"):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    indices = np.random.choice(range(len(X_transform)),num_samples, replace=False)\n",
        "    ax = sns.heatmap(X_transform.loc[indices].T, cmap=\"viridis\", cbar=True)\n",
        "    ax.set_xticks(np.arange(num_samples) + 0.5)\n",
        "    ax.set_xticklabels(y[indices], rotation=45, ha=\"right\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Datos (Clase)\")\n",
        "    plt.ylabel(\"Features Aleatorias\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7dTLkTo79ci"
      },
      "outputs": [],
      "source": [
        "plot_transformed_features(X_train_transform, y_train, num_samples=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXT7I336CoGj"
      },
      "source": [
        "#### Clasificación a partir de las RFs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2qBSpQF9dDR"
      },
      "source": [
        "\n",
        "**Ejercicio 2.2: Defina un modelo de lineal de clasificación de sklearn (e.g. [RidgeClassifierCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifierCV.html), [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) u otro de su interés). Entrénelo utilizando X_train_transform y evalúelo sobre X_test_transform. Para esto último utilice la siguiente función para visualizar la matriz de confusión obtenida (véase [este link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) para mayor información) y las métricas de clasificación del modelo (generadas utilizando el [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) de sklearn). Describa las métricas de clasificación obtenidas y utilícelas para definir qué tan bueno (o no) es su modelo.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz2nI2mVgruB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "def plot_confusion_matrix(y_test, y_pred, class_names, title=\"Matriz de Confusión\"):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicción\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.show()\n",
        "    print(\"\\n Métricas de Clasificación: \\n\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names, digits = 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRjkxxVFUIWQ"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mGllB5_vWzs"
      },
      "outputs": [],
      "source": [
        "# Rellene con su código aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BRhEKAWCgFW"
      },
      "source": [
        "#### Aproximación al problema con NNs y torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxLvp7CLC0Nf"
      },
      "source": [
        "Si bien el problema ya está relativamente \"resuelto\" con la parte anterior, es interesante también ver cómo se comportaría una red neuronal entrenada sobre nuestros vectores de _features aleatorias_ (parecido a como lo hicimos en la sección anterior)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e9nSFUGDgmH"
      },
      "source": [
        "Los modelos de clasificación de torch necesitan etiquetas numéricas, por lo que utilizamos un LabelEncoder de sklearn para codificar las etiquetas correctamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4h-72ZhDf4i"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibfO2N08EKwm"
      },
      "source": [
        "**Ejercicio 2.3: Transforme los datos de X_train_transform, X_test_transform, y_train_encoded e y_test_encoded a formato torch.tensor (cuidando que los int no se transformen en float), y cree los TensorDataset y DataLoaders necesarios para entrenar NNs con torch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsNRmn3DWtyE"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC8H2lBNEJ-k"
      },
      "outputs": [],
      "source": [
        "# Rellene con su código aquí:\n",
        "\n",
        "# Convierta los datos a torch.tensor().\n",
        "# Defínalos utilizando el parametro dtype correctamente para no tener problemas después.\n",
        "\n",
        "# Cree los TensorDataset() correspondientes.\n",
        "\n",
        "# Cree los DataLoaders() correspondientes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QZ4nALMN_1f"
      },
      "source": [
        "Considere un modelo lineal simple que se entrenará sobre las _features aleatorias_ generadas por MiniRocket:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6cWkm_gORu2"
      },
      "outputs": [],
      "source": [
        "input_dim = X_train_transform.shape[1]\n",
        "num_classes = 2\n",
        "\n",
        "model = nn.Linear(input_dim, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKHZvI2pMl1A"
      },
      "source": [
        "**Ejercicio 2.4: Utilice la misma función train_and_evaluate de la sección anterior (en esta ocasión, considerando has_get_particles = False), para entrenar este modelo. La función de pérdida en los problemas de clasificación suele ser la llamada [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), pero siéntanse libres de explorar otras posibilidades.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ACfeIMMWyL3"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muknQGYjG6L2"
      },
      "outputs": [],
      "source": [
        "# Rellene con su código aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG9v196SG04S"
      },
      "outputs": [],
      "source": [
        "# Rellene con su código aquí\n",
        "# (recuerde utilizar train_and_evaluate con has_get_particles = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcNyHlkDPbla"
      },
      "source": [
        "**Ejercicio 2.5: Grafique la evolución de la training_loss durante el entrenamiento (puede utilizar la siguiente función), y evalúe el modelo obtenido observando la matriz de confusión y las métricas de clasificación obtenidas. Modifique los hiperparámetros considerados para obtener el mejor resultado posible (e.g. pruebe agregar \"weights\" a la CrossEntropyLoss, cambiar el algoritmo de optimización, modificar la arquitectura de la red empleada, usar un \"ensamble\" de modelos, etc.). Discuta sus resultados y compárelos con los del modelo de sklearn anterior.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5JkPLyLIzh-"
      },
      "outputs": [],
      "source": [
        "def plot_train_loss(train_losses):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(train_losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Loss')\n",
        "    plt.yscale(\"log\")\n",
        "    plt.title('Evolución de la Training Loss')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-0FxA_xP529"
      },
      "source": [
        "Note que el modelo predice la \"probabilidad\" de que un dato pertenezca a cada clase determinada. Para obtener las clases predichas por el modelo, debemos acceder al argmax de dicha probabilidad (i.e. la clase de máxima probabilidad)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc_HP_HII7T9"
      },
      "outputs": [],
      "source": [
        "y_pred_NN = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF6JPLE6WTQ6"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTWbN_bNWSmd"
      },
      "outputs": [],
      "source": [
        "# Rellene con su código aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CCJ5ijEOXVD"
      },
      "source": [
        "Genial! Hemos podido entender a grandes rasgos algunos fenómenos que ocurren con modelos sobreparametrizados, aplicando algunas de estas ideas para resolver un problema del mundo real."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
